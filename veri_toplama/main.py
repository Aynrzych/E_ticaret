# veri_toplama/main.py

import json
from multiprocessing import Pool, cpu_count
import subprocess
import os
import sys
from utils import initialize_driver, scrape_akakce_base_data

# ----------------- YARDIMCI FONKSÄ°YONLAR -----------------

# TÃ¼m desteklenen sitelerin listesi
ALL_SITES = ["hepsiburada", "trendyol", "n11", "pttavm", "pazarama"]

def identify_site(url):
    """AkakÃ§e linkinin yÃ¶nlendirdiÄŸi pazar yerini belirler."""
    url = url.lower()
    if "hepsiburada.com" in url :
        return "hepsiburada"
    elif "trendyol.com" in url:
        return "trendyol"
    elif "n11.com" in url:
        return "n11"
    elif "pttavm.com" in url:
        return "pttavm"
    elif "pazarama.com" in url:
        return "pazarama"
    return "unknown"

def expand_product_tasks(product_config, check_availability=True):
    """
    Bir Ã¼rÃ¼n config'ini tÃ¼m siteler iÃ§in task'lara geniÅŸletir.
    EÄŸer 'target_sites' belirtilmiÅŸse sadece onlarÄ± kullanÄ±r,
    yoksa tÃ¼m siteleri Ã§eker.
    
    check_availability=True ise Ã¶nce AkakÃ§e'den hangi sitelerde Ã¼rÃ¼n olduÄŸunu kontrol eder.
    """
    target_sites = product_config.get('target_sites', ALL_SITES)
    # EÄŸer tek bir site belirtilmiÅŸse (eski format), onu kullan
    if 'target_site' in product_config:
        target_sites = [product_config['target_site']]
    
    # EÄŸer kontrol etmek istiyorsak, AkakÃ§e'den hangi sitelerde Ã¼rÃ¼n olduÄŸunu bul
    if check_availability and not product_config.get('target_site'):
        try:
            driver = initialize_driver()
            _, base_data = scrape_akakce_base_data(driver, product_config['url'])
            driver.quit()
            
            # AkakÃ§e'de bulunan siteleri bul
            available_vendors = set()
            vendor_name_mapping = {
                "hepsiburada": "hepsiburada",
                "trendyol": "trendyol", 
                "n11": "n11",
                "pttavm": "pttavm",
                "pazarama": "pazarama"
            }
            
            for item in base_data:
                vendor_lower = item.get('vendor_name', '').lower()
                for site_key, site_name in vendor_name_mapping.items():
                    if site_key in vendor_lower or site_name in vendor_lower:
                        available_vendors.add(site_name)
            
            # Sadece mevcut siteler iÃ§in task oluÅŸtur
            if available_vendors:
                target_sites = [site for site in target_sites if site in available_vendors]
                print(f"âœ… {product_config['product_name']}: {len(available_vendors)} sitede mevcut ({', '.join(available_vendors)})")
            else:
                print(f"âš ï¸ {product_config['product_name']}: HiÃ§bir sitede bulunamadÄ±, tÃ¼m siteler deneniyor")
        except Exception as e:
            print(f"âš ï¸ {product_config['product_name']}: AkakÃ§e kontrolÃ¼ baÅŸarÄ±sÄ±z, tÃ¼m siteler deneniyor: {e}")
    
    tasks = []
    for site in target_sites:
        task = product_config.copy()
        task['target_site'] = site
        # target_sites'i kaldÄ±r, sadece target_site kalsÄ±n
        task.pop('target_sites', None)
        tasks.append(task)
    
    return tasks

def run_scraper_script(product_config):
    """
    targets.json'dan gelen Ã¼rÃ¼ne gÃ¶re uygun site script'ini 
    subprocess ile Ã§alÄ±ÅŸtÄ±rÄ±r.
    """
    
    site_name = product_config.get('target_site', 'unknown')
    
    script_map = {
        "hepsiburada": "hb_scraping.py",
        "trendyol": "ty_scraper.py", 
        "n11": "n11_scraper.py",
        "pttavm": "ptt_scraper.py",
        "pazarama": "pazarama_scraper.py",
        # DiÄŸer siteler buraya eklenecek
    }
    
    script_file = script_map.get(site_name)
    
    if not script_file:
        return f"âš ï¸ {product_config['product_name']} iÃ§in uygun scraper ({site_name}) tanÄ±mlÄ± deÄŸil."
    
    try:
        json_arg = json.dumps(product_config)
        
        print(f"ğŸ”„ BaÅŸlatÄ±lÄ±yor: {product_config['product_name']} ({site_name} -> {script_file})")
        
        result = subprocess.run(
            [sys.executable, script_file, json_arg],
            capture_output=True,
            text=True,
            check=True 
        )
        
        return result.stdout.strip()
        
    except subprocess.CalledProcessError as e:
        error_output = e.stderr.strip() or e.stdout.strip()
        return f"âŒ Hata ({product_config['product_name']}): Script Ã§alÄ±ÅŸtÄ±rÄ±lÄ±rken sorun oluÅŸtu.\n{error_output}"
    except FileNotFoundError:
        return f"âŒ Hata: {script_file} dosyasÄ± bulunamadÄ±. LÃ¼tfen kontrol edin."
    except Exception as e:
        return f"âŒ Kritik Hata: {e}"


# ----------------- ANA Ã‡ALIÅTIRMA BLOÄU -----------------

def main_scraper_runner():
    """targets.json dosyasÄ±nÄ± yÃ¼kler ve Multiprocessing havuzunu baÅŸlatÄ±r."""
    
    try:
        with open('targets.json', 'r', encoding='utf-8') as f:
            product_list = json.load(f)
    except FileNotFoundError:
        print("âŒ Hata: targets.json bulunamadÄ±. LÃ¼tfen 'veri_toplama' klasÃ¶rÃ¼nde olduÄŸundan emin olun.")
        return
    except json.JSONDecodeError:
        print("âŒ Hata: targets.json dosyasÄ± bozuk veya geÃ§ersiz JSON formatÄ±nda.")
        return

    if not product_list:
        print("âš ï¸ targets.json dosyasÄ± boÅŸ. LÃ¼tfen en az bir Ã¼rÃ¼n URL'si ekleyin.")
        return
    
    # Her Ã¼rÃ¼n iÃ§in tÃ¼m siteleri Ã§ekmek Ã¼zere task'larÄ± geniÅŸlet
    all_tasks = []
    for product in product_list:
        tasks = expand_product_tasks(product)
        all_tasks.extend(tasks)
    
    # 500 entry iÃ§in Ã§ok fazla sÃ¼reÃ§ aÃ§mamak iÃ§in maksimum sÄ±nÄ±r koy
    num_processes = min(len(all_tasks), 10)  # Maksimum 10 paralel iÅŸlem
    print(f"=========================================================")
    print(f"ğŸ“¦ {len(product_list)} Ã¼rÃ¼n bulundu")
    print(f"ğŸ”„ Toplam {len(all_tasks)} task oluÅŸturuldu (Ã¼rÃ¼n Ã— site)")
    print(f"ğŸš€ {num_processes} paralel iÅŸlem baÅŸlatÄ±lÄ±yor...")
    print(f"=========================================================")

    with Pool(processes=num_processes) as pool:
        results = pool.map(run_scraper_script, all_tasks)

    print("\n========================= SONUÃ‡LAR =========================")
    for result in results:
        print(result)
    print("==========================================================")


if __name__ == "__main__":
    import sys
    
    # Komut satÄ±rÄ± argÃ¼manÄ±: Sadece belirli bir product_id'yi iÅŸle
    if len(sys.argv) > 1:
        target_product_id = sys.argv[1]
        print(f"ğŸ¯ Sadece '{target_product_id}' Ã¼rÃ¼nÃ¼ iÅŸlenecek...\n")
        
        try:
            with open('targets.json', 'r', encoding='utf-8') as f:
                all_products = json.load(f)
        except FileNotFoundError:
            print("âŒ Hata: targets.json bulunamadÄ±.")
            sys.exit(1)
        
        filtered_products = [p for p in all_products if p.get('product_id') == target_product_id]
        
        if not filtered_products:
            print(f"âŒ Hata: '{target_product_id}' Ã¼rÃ¼nÃ¼ targets.json'da bulunamadÄ±.")
            sys.exit(1)
        
        print(f"âœ… {len(filtered_products)} Ã¼rÃ¼n bulundu: {filtered_products[0].get('product_name', target_product_id)}\n")
        
        import shutil
        shutil.copy('targets.json', 'targets.json.backup')
        
        with open('targets.json', 'w', encoding='utf-8') as f:
            json.dump(filtered_products, f, ensure_ascii=False, indent=2)
        
        try:
            main_scraper_runner()
        finally:
            shutil.move('targets.json.backup', 'targets.json')
            print("\nâœ… targets.json orijinal haline geri yÃ¼klendi.")
    else:
        main_scraper_runner()
    
